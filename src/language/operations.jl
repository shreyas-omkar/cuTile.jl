# cuTile DSL operations

# Helper to extract compile-time shape from various tuple types
@inline _extract_shape(s::NTuple{N, Int}) where N = s
@inline _extract_shape(s::Tuple{Constant{Int, V}, Vararg{Constant{Int}}}) where V =
    (V, _extract_shape(Base.tail(s))...)
@inline _extract_shape(::Tuple{}) = ()

# Helpers to deinterleave (acc1, elem1, acc2, elem2, ...) into separate tuples
@inline _deinterleave_accs(a, e, rest...) = (a, _deinterleave_accs(rest...)...)
@inline _deinterleave_accs() = ()
@inline _deinterleave_elems(a, e, rest...) = (e, _deinterleave_elems(rest...)...)
@inline _deinterleave_elems() = ()

#=============================================================================
 Load/Store
=============================================================================#

public bid, num_blocks, num_tiles, load, store, gather, scatter

"""
Padding mode for load operations.
Use these constants with ct.load to specify out-of-bounds behavior.
"""
module PaddingMode
    const Undetermined = 0
    const Zero = 1
    const NegZero = 2
    const Nan = 3
    const PosInf = 4
    const NegInf = 5
end

"""
    bid(axis) -> Int32

Get the block ID along the given axis (1=x, 2=y, 3=z).
Returns 1-indexed block ID.
"""
@inline bid(axis::Integer) = Intrinsics.get_tile_block_id(axis - One()) + One()

"""
    num_blocks(axis) -> Int32

Get the grid size along the given axis (1=x, 2=y, 3=z).
"""
@inline num_blocks(axis::Integer) = Intrinsics.get_num_tile_blocks(axis - One())

"""
    num_tiles(arr::TileArray, axis::Integer, shape::NTuple{M, Int}) -> Int32

Get the number of tiles along a specific axis of an array, given the tile shape.
Axis is 1-indexed. Equivalent to cld(arr.sizes[axis], shape[axis]).

# Example
```julia
# For a 1024x768 matrix with 32x32 tiles:
# num_tiles(arr, 1, (32, 32)) returns cld(1024, 32) = 32
# num_tiles(arr, 2, (32, 32)) returns cld(768, 32) = 24
```
"""
@inline function num_tiles(arr::TileArray, axis::Integer, shape::NTuple{<:Any, Int})
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, shape, PaddingMode.Undetermined, nothing)
    Intrinsics.get_index_space_shape(pv, axis - One())  # convert to 0-indexed
end

"""
    load(arr::TileArray, index, shape; order=nothing, padding_mode=PaddingMode.Undetermined, latency=nothing, allow_tma=true) -> Tile

Load a tile from a TileArray at the given index with the specified shape.
Index is 1-indexed. Shape must be compile-time constant.

# Dimension Ordering
- `order`: Optional tuple specifying the logical-to-physical dimension mapping (1-indexed).
  For example, `order=(2, 1)` indicates dimension 2 is contiguous in memory,
  enabling coalesced loads from transposed/permuted arrays.
  Default: `nothing` → identity `(1, 2, ..., N)`.

# Padding Modes
- `PaddingMode.Undetermined`: Unspecified behavior for OOB access
- `PaddingMode.Zero`: Return zero for OOB elements
- `PaddingMode.NegZero`: Return negative zero for OOB elements
- `PaddingMode.Nan`: Return NaN for OOB elements
- `PaddingMode.PosInf`: Return positive infinity for OOB elements
- `PaddingMode.NegInf`: Return negative infinity for OOB elements

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default
- `allow_tma`: Whether TMA (Tensor Memory Accelerator) is allowed (default: true)

# Example
```julia
tile = ct.load(arr, (bid,), (TILE_N[],); padding_mode=ct.PaddingMode.Zero, latency=3)

# Load from a transposed array with coalesced access
tile = ct.load(arr, (bidx, bidy), (TM, TN); order=(2, 1))
```
"""
@inline function load(arr::TileArray, index, shape::NTuple{<:Any, Int};
                      order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                      padding_mode::Int=PaddingMode.Undetermined,
                      latency::Union{Int, Nothing}=nothing,
                      allow_tma::Bool=true)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, shape, padding_mode, order)
    Intrinsics.load_partition_view(pv, latency, allow_tma, promote(index...) .- One())
end

@inline function load(arr::TileArray, index::Integer, shape::NTuple{<:Any, Int};
                      order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                      padding_mode::Int=PaddingMode.Undetermined,
                      latency::Union{Int, Nothing}=nothing,
                      allow_tma::Bool=true)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, shape, padding_mode, order)
    Intrinsics.load_partition_view(pv, latency, allow_tma, (index - One(),))
end

# Load with Constant shape tuple
@inline function load(arr::TileArray, index, shape::Tuple{Vararg{Constant{Int}}};
                      order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                      padding_mode::Int=PaddingMode.Undetermined,
                      latency::Union{Int, Nothing}=nothing,
                      allow_tma::Bool=true)
    shape_val = _extract_shape(shape)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, shape_val, padding_mode, order)
    Intrinsics.load_partition_view(pv, latency, allow_tma, promote(index...) .- One())
end

# Keyword argument version
@inline function load(arr::TileArray; index, shape,
                      order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                      padding_mode::Int=PaddingMode.Undetermined,
                      latency::Union{Int, Nothing}=nothing,
                      allow_tma::Bool=true)
    shape_val = _extract_shape(shape)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, shape_val, padding_mode, order)
    Intrinsics.load_partition_view(pv, latency, allow_tma, promote(index...) .- One())
end

# Auto-reshape tile to match target array rank for store.
@inline function _reshape_for_store(tile::Tile, ::Val{N}) where {N}
    ndims(tile) <= N && return tile
    new_shape = _store_shape(Val(size(tile)), Val(N))
    reshape(tile, new_shape)
end

@generated function _store_shape(::Val{Shape}, ::Val{N}) where {Shape, N}
    M = length(Shape)
    to_drop = M - N
    singletons = [i for i in 1:M if Shape[i] == 1]
    if length(singletons) < to_drop
        error("cannot squeeze shape $Shape to rank $N: only $(length(singletons)) singleton dims but need to drop $to_drop")
    end
    drop_set = Set(singletons[1:to_drop])
    kept = tuple((Shape[i] for i in 1:M if !(i in drop_set))...)
    # Partition views require at least 1D
    if isempty(kept)
        kept = (1,)
    end
    return :($kept)
end

"""
    store(arr::TileArray, index, tile::Tile; order=nothing, latency=nothing, allow_tma=true) -> Tile

Store a tile to a TileArray at the given index. Index is 1-indexed.
Returns the stored tile (enables chaining and helps constant folding).

# Dimension Ordering
- `order`: Optional tuple specifying the logical-to-physical dimension mapping (1-indexed).
  Must match the `order` used in the corresponding `load` for permuted arrays.
  Default: `nothing` → identity `(1, 2, ..., N)`.

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default
- `allow_tma`: Whether TMA (Tensor Memory Accelerator) is allowed (default: true)
"""
@inline function store(arr::TileArray{T}, index, tile::Tile{T};
                       order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                       latency::Union{Int, Nothing}=nothing,
                       allow_tma::Bool=true) where {T}
    reshaped = _reshape_for_store(tile, Val(ndims(arr)))
    _store_reshaped(arr, reshaped, order, latency, allow_tma, promote(index...) .- One())
    return tile  # XXX: enables constant folding; remove when possible (see "constant folding" test)
end

@inline function store(arr::TileArray{T}, index::Integer, tile::Tile{T};
                       order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                       latency::Union{Int, Nothing}=nothing,
                       allow_tma::Bool=true) where {T}
    reshaped = _reshape_for_store(tile, Val(ndims(arr)))
    _store_reshaped(arr, reshaped, order, latency, allow_tma, (index - One(),))
    return tile  # XXX: enables constant folding; remove when possible (see "constant folding" test)
end

@inline function _store_reshaped(arr::TileArray{T}, tile::Tile{T},
                                 order, latency, allow_tma, indices::NTuple{<:Any, <:Integer}) where {T}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, size(tile), PaddingMode.Undetermined, order)
    Intrinsics.store_partition_view(pv, tile, latency, allow_tma, indices)
end

# Keyword argument version - dispatch to positional version
@inline function store(arr::TileArray{T}; index, tile::Tile{T},
                       order::Union{NTuple{<:Any, Int}, Nothing}=nothing,
                       latency::Union{Int, Nothing}=nothing,
                       allow_tma::Bool=true) where {T}
    store(arr, index, tile; order, latency, allow_tma)
end

"""
    gather(array::TileArray{T, 1}, indices::Tile{I, S}; latency=nothing) -> Tile{T, S}

Gather elements from a 1D array using index tile.
Indices are 1-indexed. Out-of-bounds indices return zero.

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default

# Example
```julia
base = (bid - 1) * TILE
indices = base .+ ct.arange((TILE,), Int32)
tile = ct.gather(arr, indices; latency=3)
```
"""
@inline function gather(array::TileArray{T, 1}, indices::Tile{I};
                        latency::Union{Int, Nothing}=nothing) where {T, I <: Integer}
    # Convert to 0-indexed
    indices_0 = indices .- one(I)

    # Convert to Int32 for consistency with array.sizes
    indices_i32 = astype(indices_0, Int32)

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, indices_i32)

    # Bounds mask: 0 <= indices_i32 < size
    zero_0d = Tile(Int32(0))
    size_0d = Tile(array.sizes[1])  # Already Int32
    ge_zero = indices_i32 .>= zero_0d
    lt_size = indices_i32 .< size_0d
    mask = ge_zero .& lt_size

    # Padding for OOB (zero)
    padding = broadcast_to(Tile(zero(T)), size(indices))

    Intrinsics.load_ptr_tko(ptr_tile, latency, mask, padding)
end

"""
    gather(array::TileArray{T, 2}, indices::Tuple{Tile, Tile}; latency=nothing) -> Tile{T, S}

Gather elements from a 2D array using a tuple of index tiles.
Indices are 1-indexed. Index tiles are broadcast to a common shape.

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default
"""
@inline function gather(array::TileArray{T, 2}, indices::Tuple{Tile{I0}, Tile{I1}};
                        latency::Union{Int, Nothing}=nothing) where {T, I0 <: Integer, I1 <: Integer}
    # Convert to 0-indexed
    idx0_0 = indices[1] .- one(I0)
    idx1_0 = indices[2] .- one(I1)

    # Broadcast indices to common shape
    S = broadcast_shape(size(indices[1]), size(indices[2]))
    idx0_bc = broadcast_to(idx0_0, S)
    idx1_bc = broadcast_to(idx1_0, S)

    # Convert to Int32 for linear index computation
    idx0_i32 = astype(idx0_bc, Int32)
    idx1_i32 = astype(idx1_bc, Int32)

    # Get strides and broadcast to tile shape
    stride0_0d = Tile(array.strides[1])
    stride1_0d = Tile(array.strides[2])
    stride0 = broadcast_to(stride0_0d, S)
    stride1 = broadcast_to(stride1_0d, S)

    # Compute linear index = idx0 * stride0 + idx1 * stride1
    linear_idx = idx0_i32 .* stride0 + idx1_i32 .* stride1

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, linear_idx)

    # 2D bounds mask: 0 <= idx0 < size0 && 0 <= idx1 < size1
    zero_0d = Tile(Int32(0))
    zero_bc = broadcast_to(zero_0d, S)
    size0_bc = broadcast_to(Tile(array.sizes[1]), S)
    size1_bc = broadcast_to(Tile(array.sizes[2]), S)

    mask0 = (idx0_i32 .>= zero_bc) .& (idx0_i32 .< size0_bc)
    mask1 = (idx1_i32 .>= zero_bc) .& (idx1_i32 .< size1_bc)
    mask = mask0 .& mask1

    # Padding for OOB (zero)
    padding = broadcast_to(Tile(zero(T)), S)

    Intrinsics.load_ptr_tko(ptr_tile, latency, mask, padding)
end

"""
    scatter(array::TileArray{T, 1}, indices::Tile{I, S}, tile::Tile{T, S}; latency=nothing) -> Nothing

Scatter elements to a 1D array at index tile positions.
Indices are 1-indexed. Out-of-bounds indices are ignored.

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default

# Example
```julia
base = (bid - 1) * TILE
indices = base .+ ct.arange((TILE,), Int32)
ct.scatter(arr, indices, result_tile; latency=3)
```
"""
@inline function scatter(array::TileArray{T, 1}, indices::Tile{I, S}, tile::Tile{T, S};
                         latency::Union{Int, Nothing}=nothing) where {T, I <: Integer, S}
    # Convert to 0-indexed
    indices_0 = indices .- one(I)

    # Convert to Int32 for consistency with array.sizes
    indices_i32 = astype(indices_0, Int32)

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, indices_i32)

    # Bounds mask: 0 <= indices_i32 < size
    zero_0d = Tile(Int32(0))
    size_0d = Tile(array.sizes[1])  # Already Int32
    ge_zero = indices_i32 .>= zero_0d
    lt_size = indices_i32 .< size_0d
    mask = ge_zero .& lt_size

    Intrinsics.store_ptr_tko(ptr_tile, tile, latency, mask)
end

"""
    scatter(array::TileArray{T, 2}, indices::Tuple{Tile, Tile}, tile::Tile; latency=nothing) -> Nothing

Scatter elements to a 2D array at index tile positions.
Indices are 1-indexed. Index tiles and value tile must broadcast to same shape.

# Optimization Hints
- `latency`: Optional latency hint (1-10), or nothing for compiler default
"""
@inline function scatter(array::TileArray{T, 2}, indices::Tuple{Tile{I0}, Tile{I1}}, tile::Tile{T};
                         latency::Union{Int, Nothing}=nothing) where {T, I0 <: Integer, I1 <: Integer}
    # Convert to 0-indexed
    idx0_0 = indices[1] .- one(I0)
    idx1_0 = indices[2] .- one(I1)

    # Broadcast indices to common shape
    S = broadcast_shape(broadcast_shape(size(indices[1]), size(indices[2])), size(tile))
    idx0_bc = broadcast_to(idx0_0, S)
    idx1_bc = broadcast_to(idx1_0, S)
    tile_bc = broadcast_to(tile, S)

    # Convert to Int32 for linear index computation
    idx0_i32 = astype(idx0_bc, Int32)
    idx1_i32 = astype(idx1_bc, Int32)

    # Get strides and broadcast to tile shape
    stride0_0d = Tile(array.strides[1])
    stride1_0d = Tile(array.strides[2])
    stride0 = broadcast_to(stride0_0d, S)
    stride1 = broadcast_to(stride1_0d, S)

    # Compute linear index = idx0 * stride0 + idx1 * stride1
    linear_idx = idx0_i32 .* stride0 + idx1_i32 .* stride1

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, linear_idx)

    # 2D bounds mask: 0 <= idx0 < size0 && 0 <= idx1 < size1
    zero_0d = Tile(Int32(0))
    zero_bc = broadcast_to(zero_0d, S)
    size0_bc = broadcast_to(Tile(array.sizes[1]), S)
    size1_bc = broadcast_to(Tile(array.sizes[2]), S)

    mask0 = (idx0_i32 .>= zero_bc) .& (idx0_i32 .< size0_bc)
    mask1 = (idx1_i32 .>= zero_bc) .& (idx1_i32 .< size1_bc)
    mask = mask0 .& mask1

    Intrinsics.store_ptr_tko(ptr_tile, tile_bc, latency, mask)
end

#=============================================================================
 Factory
=============================================================================#

public arange, full, zeros

"""
    arange(shape::NTuple{1, Int}, dtype::Type{T}) -> Tile{T, shape}

Create a 1D tile with values [1, 2, 3, ..., shape[1]] (1-indexed).

# Example
```julia
indices = ct.arange((16,), Int32)  # Creates Tile with [1, 2, 3, ..., 16]
```
"""
@inline arange(shape::NTuple{1, Int}, ::Type{T}) where {T} =
    Intrinsics.iota(shape, T) .+ one(T)

# Helper for integer constant shape
@inline arange(shape::Tuple{Constant{Int, V}}, ::Type{T}) where {V, T} =
    arange((V,), T)

"""
    full(shape::NTuple{N, Int}, value, dtype::Type{T}) -> Tile{T, shape}

Create a tile filled with a constant value.

# Example
```julia
ones_tile = ct.full((32, 32), 1.0f0, Float32)
```
"""
@inline full(shape::NTuple{N, Int}, value, ::Type{T}) where {N, T} =
    Intrinsics.constant(shape, value, T)

"""
    zeros(shape::NTuple{N, Int}, dtype::Type{T}) -> Tile{T, shape}

Create a tile filled with zeros.

# Example
```julia
zeros_tile = ct.zeros((32, 32), Float32)
```
"""
@inline zeros(shape::NTuple{N, Int}, ::Type{T}) where {N, T} =
    full(shape, zero(T), T)

#=============================================================================
 Shape & DType
=============================================================================#

public cat, broadcast_to, astype

"""
    cat(tiles::Tuple{Tile, Tile}, axis::Int) -> Tile

Concatenate two tiles along the specified axis (1-indexed).
Supports negative axis (e.g., -1 for last dimension).

# Example
```julia
tile_a = ct.load(arr_a, (1,), (4, 8))  # Shape (4, 8)
tile_b = ct.load(arr_b, (1,), (4, 8))  # Shape (4, 8)
# Concatenate along axis 1: (4, 8) + (4, 8) -> (8, 8)
combined = ct.cat((tile_a, tile_b), 1)
# Concatenate along axis -1 (last): (4, 8) + (4, 8) -> (4, 16)
combined_last = ct.cat((tile_a, tile_b), -1)
```
"""
@inline function cat(tiles::Tuple{Tile{T, S1}, Tile{T, S2}}, axis::Int) where {T, S1, S2}
    axis0 = axis < 0 ? axis : axis - 1
    Intrinsics.cat(tiles, axis0)
end
@inline function cat(tiles::Tuple{Tile{T, S1}, Tile{T, S2}}, ::Val{Axis}) where {T, S1, S2, Axis}
    axis0 = Axis < 0 ? Axis : Axis - 1
    Intrinsics.cat(tiles, axis0)
end

"""
    broadcast_to(tile::Tile{T, S}, shape::NTuple{N, Int}) -> Tile{T, shape}

Explicitly broadcast a tile to a target shape.

# Example
```julia
row = ct.load(arr, (1, 1), (1, 128))  # Shape (1, 128)
expanded = ct.broadcast_to(row, (64, 128))  # Shape (64, 128)
```
"""
@inline broadcast_to(tile::Tile{T}, shape::NTuple{<:Any, Int}) where {T} =
    Intrinsics.broadcast(tile, shape)

"""
    reshape(tile::Tile{T, S}, shape::NTuple{N, Int}) -> Tile{T, shape}

Reshape a tile to a new shape. The total number of elements must remain the same.

# Example
```julia
tile = ct.load(arr, (1, 1), (4, 8))  # Shape (4, 8), 32 elements
reshaped = reshape(tile, (2, 16))  # Shape (2, 16), still 32 elements
```
"""
@inline Base.reshape(tile::Tile{T}, shape::NTuple{<:Any, Int}) where {T} =
    Intrinsics.reshape(tile, shape)

"""
    permutedims(tile::Tile{T, S}, perm) -> Tile{T, permuted_shape}

Permute the dimensions of a tile according to the given permutation.
The permutation uses 1-indexed axes (Julia convention).

# Example
```julia
tile = ct.load(arr, (1, 1, 1), (2, 3, 4))  # Shape (2, 3, 4)
permuted = permutedims(tile, (3, 1, 2))    # Shape (4, 2, 3)
```
"""
@inline Base.permutedims(tile::Tile{T}, perm::NTuple{<:Any, Int}) where {T} =
    Intrinsics.permute(tile, map(p -> p - 1, perm))
@inline Base.permutedims(tile::Tile{T}, ::Val{Perm}) where {T, Perm} =
    Intrinsics.permute(tile, map(p -> p - 1, Perm))

"""
    permutedims(tile::Tile{T, (M, N)}) -> Tile{T, (N, M)}

Permute a 2D tile, swapping its dimensions. Defaults to permutation `(2, 1)`.

Differs from `transpose` in that the operation is not recursive. For tiles
of numeric element types, the two operations are equivalent.

---

    permutedims(tile::Tile{T, (N,)}) -> Tile{T, (1, N)}

Reshape a 1D tile into a `1 × N` row tile.

Differs from `transpose` in that the operation is not recursive.
"""
@generated function Base.permutedims(tile::T) where {T <: Tile}
    n = ndims(T)
    first_dim = n >= 1 ? size(T, 1) : nothing

    if n == 2
        return :(Intrinsics.permute(tile, (1, 0)))
    elseif n == 1
        return :(Intrinsics.reshape(tile, (1, $first_dim)))
    else
        return :(throw(ArgumentError("permutedims(tile) only works for 1D or 2D tiles")))
    end
end

"""
    transpose(tile::Tile{T, (M, N)}) -> Tile{T, (N, M)}

Transpose a 2D tile, swapping its dimensions.
Equivalent to `permute(tile, (2, 1))`.
"""
@inline Base.transpose(tile::Tile{T}) where {T} =
    Intrinsics.transpose(tile)

"""
    astype(tile::Tile{T1, Shape}, ::Type{T2}) -> Tile{T2, Shape}

Convert a tile's element type from T1 to T2.

# Example
```julia
acc = ct.full((64, 64), 0.0f0, Float32)
result = ct.astype(acc, ct.TFloat32)  # Convert to TF32 for tensor cores
```
"""
@inline astype(tile::Tile{T1, Shape}, ::Type{T2}) where {T1, Shape, T2} =
    Intrinsics.astype(tile, T2)

# Julia-style convert syntax
@inline Base.convert(::Type{Tile{T2}}, tile::Tile{T1, Shape}) where {T1, T2, Shape} =
    astype(tile, T2)

#=============================================================================
 Reduction
=============================================================================#

"""
    map(f, a::Tile{<:Any,S}, rest::Tile{<:Any,S}...) -> Tile

Apply function `f` element-wise across one or more same-shaped tiles.
The function `f` must be a zero-size callable (singleton or capture-free lambda).
All tiles must have the same shape `S` — use broadcasting (`.+` etc.) or explicit
`broadcast_to` for shape-mismatched operands.

# Examples
```julia
result = map(abs, tile)           # Element-wise absolute value
result = map(x -> x * x, tile)   # Element-wise square
result = map(+, a, b)            # Element-wise addition (same shape required)
```
"""
@inline function Base.map(f, a::Tile{<:Any,S}, rest::Tile{<:Any,S}...) where {S}
    Intrinsics.from_scalar(f(Intrinsics.to_scalar(a), map(Intrinsics.to_scalar, rest)...), S)
end

"""
    mapreduce(identity, f, tile::Tile{T,S}; dims, init) -> Tile{T, reduced_shape}
    mapreduce(f, op, tile::Tile{T,S}; dims, init) -> Tile{T, reduced_shape}
    mapreduce(identity, f, tile1, tile2, ...; dims, init) -> Tuple{Tile...}

Reduce one or more tiles along `dims` using binary function `f`.

The single-tile form reduces a single tile along `dims` with identity element
`init`. The multi-tile form reduces several same-shaped tiles simultaneously:
`f` receives two tuples `(accumulators...)` and `(elements...)` and must return
a tuple of updated accumulators. Each tile requires a corresponding entry in the
`init` tuple.

When a non-identity map function is provided, it is applied element-wise via
`map` before reduction. The map function must be type-preserving.

# Examples
```julia
sums = mapreduce(identity, +, tile; dims=2, init=zero(Float32))
sum_of_squares = mapreduce(x -> x * x, +, tile; dims=2, init=zero(Float32))
sum_of_abs = mapreduce(abs, +, tile; dims=2, init=zero(Float32))

# Simultaneous reduction of two tiles
vals, idxs = mapreduce(identity, reducer, vals_tile, idx_tile;
                       dims=1, init=(typemin(Float32), Int32(0)))
```
"""
@inline function Base.mapreduce(::typeof(identity), f, tile::Tile{T,S};
                                dims::Integer, init) where {T<:Number, S}
    Intrinsics.reduce((tile,), dims - 1, f, (T(init),))[1]
end

@inline function Base.mapreduce(f, op, tile::Tile{T,S};
                                dims::Integer, init) where {T<:Number, S}
    reduce(op, map(f, tile); dims, init)
end

@inline function Base.mapreduce(::typeof(identity), f,
                                tile1::Tile{<:Any,S}, tile2::Tile{<:Any,S},
                                tiles::Tile{<:Any,S}...;
                                dims::Integer,
                                init::Tuple{Any, Any, Vararg{Any}}) where {S}
    all_tiles = (tile1, tile2, tiles...)
    function _combiner(args...)
        f(_deinterleave_accs(args...), _deinterleave_elems(args...))
    end
    Intrinsics.reduce(all_tiles, dims - 1, _combiner, init)
end

"""
    reduce(f, tile::Tile{T,S}; dims::Integer, init) -> Tile{T, reduced_shape}

Reduce a tile along the specified dimension using binary function `f` with
identity element `init`. The `dims` axis is 1-indexed. The reduced dimension
becomes size 1 (Julia semantics).

Supported functions: `+`, `*`, `max`, `min`.

# Example
```julia
sums = reduce(+, tile; dims=2, init=zero(Float32))
```
"""
@inline function Base.reduce(f, tile::Tile{T,S}; dims::Integer, init) where {T<:Number, S}
    mapreduce(identity, f, tile; dims, init)
end

"""
    sum(tile::Tile{T,S}; dims) -> Tile{T, reduced_shape}

Sum reduction along the specified axis/axes (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.sum(tile::Tile{T,S}; dims) where {T<:Number, S} =
    reduce(+, tile; dims, init=zero(T))

"""
    prod(tile::Tile{T,S}; dims) -> Tile{T, reduced_shape}

Product reduction along the specified axis/axes (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.prod(tile::Tile{T,S}; dims) where {T<:Number, S} =
    reduce(*, tile; dims, init=one(T))

"""
    maximum(tile::Tile{T,S}; dims) -> Tile{T, reduced_shape}

Maximum reduction along the specified axis/axes (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.maximum(tile::Tile{T,S}; dims) where {T<:Number, S} =
    reduce(max, tile; dims, init=typemin(T))

"""
    minimum(tile::Tile{T,S}; dims) -> Tile{T, reduced_shape}

Minimum reduction along the specified axis/axes (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.minimum(tile::Tile{T,S}; dims) where {T<:Number, S} =
    reduce(min, tile; dims, init=typemax(T))

"""
    any(tile::Tile{Bool,S}; dims) -> Tile{Bool, reduced_shape}

Logical OR reduction along the specified axis (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.any(tile::Tile{Bool,S}; dims::Integer) where {S} =
    reduce(|, tile; dims, init=false)

"""
    all(tile::Tile{Bool,S}; dims) -> Tile{Bool, reduced_shape}

Logical AND reduction along the specified axis (1-indexed).
Reduced dimensions become size 1.
"""
@inline Base.all(tile::Tile{Bool,S}; dims::Integer) where {S} =
    reduce(&, tile; dims, init=true)

"""
    count(tile::Tile{Bool,S}; dims) -> Tile{Int32, reduced_shape}

Count true elements along `dims` (1-indexed). Apply predicates via
broadcasting before calling count:

# Example
```julia
n_positive = count(tile .> 0.0f0; dims=1)
```
"""
@inline function Base.count(tile::Tile{Bool,S}; dims::Integer) where {S}
    sum(astype(tile, Int32); dims)
end

"""
    argmax(tile::Tile{T,S}; dims) -> Tile{Int32, reduced_shape}

Return 1-indexed positions of maximum values along `dims`.
Ties are broken by smallest index.

# Example
```julia
indices = argmax(tile; dims=2)  # Column indices of max per row
```
"""
@inline function Base.argmax(tile::Tile{T}; dims::Integer) where {T<:Number}
    n = size(tile, dims)
    indices = reshape(Intrinsics.iota((n,), Int32),
                      ntuple(i -> i == dims ? n : 1, ndims(tile)))
    indices = broadcast_to(indices, size(tile))

    function reducer(accs, elems)
        val_acc, idx_acc = accs
        val_elem, idx_elem = elems
        strict = val_acc > val_elem
        eq = val_acc == val_elem
        cond = strict | (eq & (idx_acc < idx_elem))
        (ifelse(cond, val_acc, val_elem),
         ifelse(cond, idx_acc, idx_elem))
    end
    _, idx = mapreduce(identity, reducer, tile, indices; dims, init=(typemin(T), Int32(0)))
    idx .+ one(Int32)
end

"""
    argmin(tile::Tile{T,S}; dims) -> Tile{Int32, reduced_shape}

Return 1-indexed positions of minimum values along `dims`.
Ties are broken by smallest index.

# Example
```julia
indices = argmin(tile; dims=2)  # Column indices of min per row
```
"""
@inline function Base.argmin(tile::Tile{T}; dims::Integer) where {T<:Number}
    n = size(tile, dims)
    indices = reshape(Intrinsics.iota((n,), Int32),
                      ntuple(i -> i == dims ? n : 1, ndims(tile)))
    indices = broadcast_to(indices, size(tile))

    function reducer(accs, elems)
        val_acc, idx_acc = accs
        val_elem, idx_elem = elems
        strict = val_elem > val_acc
        eq = val_acc == val_elem
        cond = strict | (eq & (idx_acc < idx_elem))
        (ifelse(cond, val_acc, val_elem),
         ifelse(cond, idx_acc, idx_elem))
    end
    _, idx = mapreduce(identity, reducer, tile, indices; dims, init=(typemax(T), Int32(0)))
    idx .+ one(Int32)
end


"""
    dropdims(tile::Tile{T,S}; dims) -> Tile{T, squeezed_shape}

Remove singleton dimensions from a tile. The specified dimensions must have
size 1. This is the inverse of the dimension-preserving behavior of `sum`,
`prod`, `maximum`, and `minimum`.

# Example
```julia
sums = sum(tile; dims=2)           # (64, 1)
squeezed = dropdims(sums; dims=2)  # (64,)
```
"""
@inline Base.dropdims(tile::Tile; dims::Integer) =
    _dropdims(tile, Val(dims))

@inline function _dropdims(tile::Tile, ::Val{D}) where {D}
    new_shape = ntuple(i -> i < D ? size(tile, i) : size(tile, i + 1), Val(ndims(tile) - 1))
    reshape(tile, new_shape)
end

#=============================================================================
 Scan (Prefix Sum) Operations
=============================================================================#

"""
    accumulate(f, tile::Tile{T,S}; dims::Integer, init, rev::Bool=false) -> Tile{T, S}

Scan (prefix sum) along the specified dimension using binary function `f`.
The `dims` axis is 1-indexed.

Supported functions: `+`, `*`, `max`, `min`.
"""
@inline function Base.accumulate(f, tile::Tile{T,S}; dims::Integer,
                                 init, rev::Bool=false) where {T<:Number, S}
    Intrinsics.scan((tile,), dims - 1, f, (T(init),), rev)[1]
end

"""
    cumsum(tile::Tile{T,S}; dims::Integer, rev::Bool=false) -> Tile{T, S}

Cumulative sum along the specified axis (1-indexed).
"""
@inline Base.cumsum(tile::Tile{T,S}; dims::Integer,
                    rev::Bool=false) where {T<:Number, S} =
    accumulate(+, tile; dims, init=zero(T), rev)

"""
    cumprod(tile::Tile{T,S}; dims::Integer, rev::Bool=false) -> Tile{T, S}

Cumulative product along the specified axis (1-indexed).
"""
@inline Base.cumprod(tile::Tile{T,S}; dims::Integer,
                     rev::Bool=false) where {T<:Number, S} =
    accumulate(*, tile; dims, init=one(T), rev)

#=============================================================================
 Matrix multiplication
=============================================================================#

# Matrix multiply-accumulate: muladd(a, b, acc) = a * b + acc
@inline Base.muladd(a::Tile{T1, SA}, b::Tile{T2, SB}, acc::Tile{T3, SC}) where {T1, T2, T3, SA, SB, SC} =
    Intrinsics.mma(a, b, acc)

# Matrix multiplication (A * B like Julia arrays)
# Note: SA, SB type parameters required to avoid ambiguity with scalar*tile methods during codegen
@inline function Base.:(*)(a::Tile{T1, SA}, b::Tile{T2, SB}) where {T1, T2, SA, SB}
    _matmul(a, b, Val(ndims(a)))
end

# 2D matmul: (M, K) × (K, N) → (M, N)
@inline function _matmul(a::Tile{T1}, b::Tile, ::Val{2}) where {T1}
    M = size(a, 1)
    N = size(b, 2)
    acc = zeros((M, N), T1)
    muladd(a, b, acc)
end

# 3D batched matmul: (B, M, K) × (B, K, N) → (B, M, N)
@inline function _matmul(a::Tile{T1}, b::Tile, ::Val{3}) where {T1}
    B = max(size(a, 1), size(b, 1))  # Broadcast batch dimension
    M = size(a, 2)
    N = size(b, 3)
    acc = zeros((B, M, N), T1)
    muladd(a, b, acc)
end

#=============================================================================
 Selection
=============================================================================#

public where, extract

"""
    where(cond::Tile{Bool}, x, y) -> Tile

Element-wise conditional selection: returns x where cond is true, y otherwise.
Similar to numpy.where() or torch.where(). Supports broadcasting and scalar arguments.

# Example
```julia
mask = tile_a .> tile_b  # Boolean tile
result = ct.where(mask, tile_a, tile_b)  # Element-wise max
result = ct.where(mask, tile_a, 0.0f0)  # Zero out where mask is false
```
"""
where(cond, x, y) = ifelse.(cond, x, y)

"""
    extract(tile::Tile{T, S}, index::NTuple{N, Int}, shape::NTuple{N, Int}) -> Tile{T, shape}

Extract a sub-tile from a tile at the given slice indices.

**IMPORTANT:** The `index` parameter specifies SLICE INDICES, not element offsets!

For each dimension, the source tile is divided into `S[i] ÷ shape[i]` non-overlapping slices.
The `index[i]` selects which slice to extract (1-indexed).

# Example: Extracting quadrants from an 8×8 tile
```julia
tile = ct.load(arr, (1, 1), (8, 8))
# 8÷4 = 2 slices per dimension, so valid indices are {1, 2} × {1, 2}
tl = ct.extract(tile, (1, 1), (4, 4))  # Top-left (rows 1-4, cols 1-4)
bl = ct.extract(tile, (2, 1), (4, 4))  # Bottom-left (rows 5-8, cols 1-4)
tr = ct.extract(tile, (1, 2), (4, 4))  # Top-right (rows 1-4, cols 5-8)
br = ct.extract(tile, (2, 2), (4, 4))  # Bottom-right (rows 5-8, cols 5-8)
```
"""
@inline extract(tile::Tile{T}, index::NTuple{<:Any, Int}, shape::NTuple{<:Any, Int}) where {T} =
    Intrinsics.extract(tile, map(i -> i - 1, index), shape)
@inline extract(tile::Tile{T}, ::Val{Index}, ::Val{Shape}) where {T, Index, Shape} =
    Intrinsics.extract(tile, map(i -> i - 1, Index), Shape)
