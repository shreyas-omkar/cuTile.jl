#=============================================================================
 User-Facing Operations

 This file provides the Julia-style 1-indexed API that users interact with.
 Each function calls the corresponding Intrinsics.* function with appropriate
 index conversions (Julia 1-indexed -> Tile IR 0-indexed).

 Organized per cuTile Python documentation structure:
 https://docs.nvidia.com/cuda/cutile-python/operations.html
=============================================================================#

# Helper: subtract 1 from each element of a tuple, preserving element types
@inline _sub1(index::Tuple) = map(i -> i - one(i), index)

# Helper to extract compile-time shape from various tuple types
@inline _extract_shape(s::NTuple{N, Int}) where N = s
@inline _extract_shape(s::Tuple{Constant{Int, V}, Vararg{Constant{Int}}}) where V =
    (V, _extract_shape(Base.tail(s))...)
@inline _extract_shape(::Tuple{}) = ()

#=============================================================================
 Load/Store
=============================================================================#

public bid, num_blocks, num_tiles, load, store, gather, scatter

"""
Padding mode for load operations.
Use these constants with ct.load to specify out-of-bounds behavior.
"""
module PaddingMode
    const Undetermined = 0
    const Zero = 1
    const NegZero = 2
    const Nan = 3
    const PosInf = 4
    const NegInf = 5
end

"""
    bid(axis) -> Int32

Get the block ID along the given axis (1=x, 2=y, 3=z).
Returns 1-indexed block ID.
"""
@inline bid(axis::Integer)::Int32 = Intrinsics.get_tile_block_id(axis - one(axis)) + Int32(1)

"""
    num_blocks(axis) -> Int32

Get the grid size along the given axis (1=x, 2=y, 3=z).
"""
@inline num_blocks(axis::Integer)::Int32 = Intrinsics.get_num_tile_blocks(axis - one(axis))

"""
    num_tiles(arr::TileArray, axis::Integer, shape::NTuple{M, Int}) -> Int32

Get the number of tiles along a specific axis of an array, given the tile shape.
Axis is 1-indexed. Equivalent to cdiv(arr.sizes[axis], shape[axis]).

# Example
```julia
# For a 1024x768 matrix with 32x32 tiles:
# num_tiles(arr, 1, (32, 32)) returns cdiv(1024, 32) = 32
# num_tiles(arr, 2, (32, 32)) returns cdiv(768, 32) = 24
```
"""
@inline function num_tiles(arr::TileArray{T, N}, axis::Integer, shape::NTuple{M, Int})::Int32 where {T, N, M}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(shape), PaddingMode.Undetermined)
    Intrinsics.get_index_space_shape(pv, axis - 1)  # convert to 0-indexed
end

"""
    load(arr::TileArray, index, shape; padding_mode=PaddingMode.Undetermined) -> Tile

Load a tile from a TileArray at the given index with the specified shape.
Index is 1-indexed. Shape must be compile-time constant.

# Padding Modes
- `PaddingMode.Undetermined`: Unspecified behavior for OOB access
- `PaddingMode.Zero`: Return zero for OOB elements
- `PaddingMode.NegZero`: Return negative zero for OOB elements
- `PaddingMode.Nan`: Return NaN for OOB elements
- `PaddingMode.PosInf`: Return positive infinity for OOB elements
- `PaddingMode.NegInf`: Return negative infinity for OOB elements

# Example
```julia
tile = ct.load(arr, (bid,), (TILE_N[],); padding_mode=ct.PaddingMode.Zero)
```
"""
@inline function load(arr::TileArray{T, N}, index, shape::NTuple{M, Int};
                      padding_mode::Int=PaddingMode.Undetermined) where {T, N, M}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(shape), padding_mode)
    Intrinsics.load_partition_view(pv, _sub1(index)...)
end

@inline function load(arr::TileArray{T, N}, index::Integer, shape::NTuple{M, Int};
                      padding_mode::Int=PaddingMode.Undetermined) where {T, N, M}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(shape), padding_mode)
    Intrinsics.load_partition_view(pv, index - one(index))
end

# Load with Constant shape tuple
@inline function load(arr::TileArray{T, N}, index, shape::Tuple{Vararg{Constant{Int}}};
                      padding_mode::Int=PaddingMode.Undetermined) where {T, N}
    shape_val = _extract_shape(shape)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(shape_val), padding_mode)
    Intrinsics.load_partition_view(pv, _sub1(index)...)
end

# Keyword argument version
@inline function load(arr::TileArray{T, N}; index, shape,
                      padding_mode::Int=PaddingMode.Undetermined) where {T, N}
    shape_val = _extract_shape(shape)
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(shape_val), padding_mode)
    Intrinsics.load_partition_view(pv, _sub1(index)...)
end

"""
    store(arr::TileArray, index, tile::Tile) -> Nothing

Store a tile to a TileArray at the given index. Index is 1-indexed.
"""
# Regular N-D tiles (N >= 1)
@inline function store(arr::TileArray{T, N}, index, tile::Tile{T, Shape}) where {T, N, Shape}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(Shape), PaddingMode.Undetermined)
    Intrinsics.store_partition_view(pv, tile, _sub1(index)...)
end

@inline function store(arr::TileArray{T, N}, index::Integer, tile::Tile{T, Shape}) where {T, N, Shape}
    tv = Intrinsics.make_tensor_view(arr)
    pv = Intrinsics.make_partition_view(tv, Val(Shape), PaddingMode.Undetermined)
    Intrinsics.store_partition_view(pv, tile, index - one(index))
end

# Special case for 0D (scalar) tiles - reshape to 1D for partition view
@inline function store(arr::TileArray{T, N}, index, tile::Tile{T, ()}) where {T, N}
    tv = Intrinsics.make_tensor_view(arr)
    # Reshape 0D tile to 1D (partition views require at least 1D)
    tile_1d = Intrinsics.reshape(tile, Val((1,)))
    pv = Intrinsics.make_partition_view(tv, Val((1,)), PaddingMode.Undetermined)
    Intrinsics.store_partition_view(pv, tile_1d, _sub1(index)...)
end

@inline function store(arr::TileArray{T, N}, index::Integer, tile::Tile{T, ()}) where {T, N}
    tv = Intrinsics.make_tensor_view(arr)
    tile_1d = Intrinsics.reshape(tile, Val((1,)))
    pv = Intrinsics.make_partition_view(tv, Val((1,)), PaddingMode.Undetermined)
    Intrinsics.store_partition_view(pv, tile_1d, index - one(index))
end

# Keyword argument version - dispatch to positional version
@inline function store(arr::TileArray{T, N}; index, tile::Tile{T, Shape}) where {T, N, Shape}
    store(arr, index, tile)
end

"""
    gather(array::TileArray{T, 1}, indices::Tile{I, S}) -> Tile{T, S}

Gather elements from a 1D array using index tile.
Indices are 1-indexed. Out-of-bounds indices return zero.

# Example
```julia
base = (bid - 1) * TILE
indices = base .+ ct.arange((TILE,), Int32)
tile = ct.gather(arr, indices)
```
"""
@inline function gather(array::TileArray{T, 1}, indices::Tile{I, S}) where {T, I <: Integer, S}
    # Convert to 0-indexed
    indices_0 = indices .- one(I)

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, indices_0)

    # Bounds mask: 0 <= indices_0 < size
    zero_0d = Tile(zero(I))
    size_0d = Tile(array.sizes[1])
    ge_zero = tile_ge(indices_0, zero_0d)
    lt_size = tile_lt(indices_0, size_0d)
    mask = ge_zero & lt_size

    # Padding for OOB (zero)
    padding = broadcast_to(Tile(zero(T)), S)

    Intrinsics.load_ptr_tko(ptr_tile, mask, padding)
end

"""
    gather(array::TileArray{T, 2}, indices::Tuple{Tile, Tile}) -> Tile{T, S}

Gather elements from a 2D array using a tuple of index tiles.
Indices are 1-indexed. Index tiles are broadcast to a common shape.
"""
@inline function gather(array::TileArray{T, 2}, indices::Tuple{Tile{I0, S0}, Tile{I1, S1}}) where {T, I0 <: Integer, I1 <: Integer, S0, S1}
    # Convert to 0-indexed
    idx0_0 = indices[1] .- one(I0)
    idx1_0 = indices[2] .- one(I1)

    # Broadcast indices to common shape
    S = broadcast_shape(S0, S1)
    idx0_bc = broadcast_to(idx0_0, S)
    idx1_bc = broadcast_to(idx1_0, S)

    # Convert to Int32 for linear index computation
    idx0_i32 = astype(idx0_bc, Int32)
    idx1_i32 = astype(idx1_bc, Int32)

    # Get strides and broadcast to tile shape
    stride0_0d = Tile(array.strides[1])
    stride1_0d = Tile(array.strides[2])
    stride0 = broadcast_to(stride0_0d, S)
    stride1 = broadcast_to(stride1_0d, S)

    # Compute linear index = idx0 * stride0 + idx1 * stride1
    linear_idx = idx0_i32 * stride0 + idx1_i32 * stride1

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, linear_idx)

    # 2D bounds mask: 0 <= idx0 < size0 && 0 <= idx1 < size1
    zero_0d = Tile(Int32(0))
    zero_bc = broadcast_to(zero_0d, S)
    size0_bc = broadcast_to(Tile(array.sizes[1]), S)
    size1_bc = broadcast_to(Tile(array.sizes[2]), S)

    mask0 = tile_ge(idx0_i32, zero_bc) & tile_lt(idx0_i32, size0_bc)
    mask1 = tile_ge(idx1_i32, zero_bc) & tile_lt(idx1_i32, size1_bc)
    mask = mask0 & mask1

    # Padding for OOB (zero)
    padding = broadcast_to(Tile(zero(T)), S)

    Intrinsics.load_ptr_tko(ptr_tile, mask, padding)
end

"""
    scatter(array::TileArray{T, 1}, indices::Tile{I, S}, tile::Tile{T, S}) -> Nothing

Scatter elements to a 1D array at index tile positions.
Indices are 1-indexed. Out-of-bounds indices are ignored.

# Example
```julia
base = (bid - 1) * TILE
indices = base .+ ct.arange((TILE,), Int32)
ct.scatter(arr, indices, result_tile)
```
"""
@inline function scatter(array::TileArray{T, 1}, indices::Tile{I, S}, tile::Tile{T, S}) where {T, I <: Integer, S}
    # Convert to 0-indexed
    indices_0 = indices .- one(I)

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, indices_0)

    # Bounds mask: 0 <= indices_0 < size
    zero_0d = Tile(zero(I))
    size_0d = Tile(array.sizes[1])
    ge_zero = tile_ge(indices_0, zero_0d)
    lt_size = tile_lt(indices_0, size_0d)
    mask = ge_zero & lt_size

    Intrinsics.store_ptr_tko(ptr_tile, tile, mask)
end

"""
    scatter(array::TileArray{T, 2}, indices::Tuple{Tile, Tile}, tile::Tile) -> Nothing

Scatter elements to a 2D array at index tile positions.
Indices are 1-indexed. Index tiles and value tile must broadcast to same shape.
"""
@inline function scatter(array::TileArray{T, 2}, indices::Tuple{Tile{I0, S0}, Tile{I1, S1}}, tile::Tile{T, Stile}) where {T, I0 <: Integer, I1 <: Integer, S0, S1, Stile}
    # Convert to 0-indexed
    idx0_0 = indices[1] .- one(I0)
    idx1_0 = indices[2] .- one(I1)

    # Broadcast indices to common shape (include value tile shape)
    S = broadcast_shape(broadcast_shape(S0, S1), Stile)
    idx0_bc = broadcast_to(idx0_0, S)
    idx1_bc = broadcast_to(idx1_0, S)
    tile_bc = broadcast_to(tile, S)

    # Convert to Int32 for linear index computation
    idx0_i32 = astype(idx0_bc, Int32)
    idx1_i32 = astype(idx1_bc, Int32)

    # Get strides and broadcast to tile shape
    stride0_0d = Tile(array.strides[1])
    stride1_0d = Tile(array.strides[2])
    stride0 = broadcast_to(stride0_0d, S)
    stride1 = broadcast_to(stride1_0d, S)

    # Compute linear index = idx0 * stride0 + idx1 * stride1
    linear_idx = idx0_i32 * stride0 + idx1_i32 * stride1

    # Compute pointer tile
    ptr_tile = Intrinsics.offset(array.ptr, linear_idx)

    # 2D bounds mask: 0 <= idx0 < size0 && 0 <= idx1 < size1
    zero_0d = Tile(Int32(0))
    zero_bc = broadcast_to(zero_0d, S)
    size0_bc = broadcast_to(Tile(array.sizes[1]), S)
    size1_bc = broadcast_to(Tile(array.sizes[2]), S)

    mask0 = tile_ge(idx0_i32, zero_bc) & tile_lt(idx0_i32, size0_bc)
    mask1 = tile_ge(idx1_i32, zero_bc) & tile_lt(idx1_i32, size1_bc)
    mask = mask0 & mask1

    Intrinsics.store_ptr_tko(ptr_tile, tile_bc, mask)
end

#=============================================================================
 Factory
=============================================================================#

public arange, full, zeros

"""
    arange(shape::NTuple{1, Int}, dtype::Type{T}) -> Tile{T, shape}

Create a 1D tile with values [1, 2, 3, ..., shape[1]] (1-indexed).

# Example
```julia
indices = ct.arange((16,), Int32)  # Creates Tile with [1, 2, 3, ..., 16]
```
"""
@inline arange(shape::NTuple{1, Int}, ::Type{T}) where {T} =
    Intrinsics.iota(shape, T) .+ one(T)

# Helper for integer constant shape
@inline arange(shape::Tuple{Constant{Int, V}}, ::Type{T}) where {V, T} =
    arange((V,), T)

"""
    full(shape::NTuple{N, Int}, value, dtype::Type{T}) -> Tile{T, shape}

Create a tile filled with a constant value.

# Example
```julia
ones_tile = ct.full((32, 32), 1.0f0, Float32)
```
"""
@inline full(shape::NTuple{N, Int}, value, ::Type{T}) where {N, T} =
    Intrinsics.constant(shape, value, T)

"""
    zeros(shape::NTuple{N, Int}, dtype::Type{T}) -> Tile{T, shape}

Create a tile filled with zeros.

# Example
```julia
zeros_tile = ct.zeros((32, 32), Float32)
```
"""
@inline zeros(shape::NTuple{N, Int}, ::Type{T}) where {N, T} =
    full(shape, zero(T), T)

#=============================================================================
 Shape & DType
=============================================================================#

public cat, broadcast_to, reshape, permute, transpose, astype

"""
    cat(tiles::Tuple{Tile, Tile}, axis::Int) -> Tile

Concatenate two tiles along the specified axis (1-indexed).
Supports negative axis (e.g., -1 for last dimension).

# Example
```julia
tile_a = ct.load(arr_a, (1,), (4, 8))  # Shape (4, 8)
tile_b = ct.load(arr_b, (1,), (4, 8))  # Shape (4, 8)
# Concatenate along axis 1: (4, 8) + (4, 8) -> (8, 8)
combined = ct.cat((tile_a, tile_b), 1)
# Concatenate along axis -1 (last): (4, 8) + (4, 8) -> (4, 16)
combined_last = ct.cat((tile_a, tile_b), -1)
```
"""
@inline function cat(tiles::Tuple{Tile{T, S1}, Tile{T, S2}}, axis::Int) where {T, S1, S2}
    axis0 = axis < 0 ? axis : axis - 1
    Intrinsics.cat(tiles, Val(axis0))
end
@inline function cat(tiles::Tuple{Tile{T, S1}, Tile{T, S2}}, ::Val{Axis}) where {T, S1, S2, Axis}
    axis0 = Axis < 0 ? Axis : Axis - 1
    Intrinsics.cat(tiles, Val(axis0))
end

"""
    broadcast_to(tile::Tile{T, S}, shape::NTuple{N, Int}) -> Tile{T, shape}

Explicitly broadcast a tile to a target shape.

# Example
```julia
row = ct.load(arr, (1, 1), (1, 128))  # Shape (1, 128)
expanded = ct.broadcast_to(row, (64, 128))  # Shape (64, 128)
```
"""
@inline broadcast_to(tile::Tile{T, S}, shape::NTuple{N, Int}) where {T, S, N} =
    Intrinsics.broadcast(tile, Val(shape))

"""
    reshape(tile::Tile{T, S}, shape::NTuple{N, Int}) -> Tile{T, shape}

Reshape a tile to a new shape. The total number of elements must remain the same.

# Example
```julia
tile = ct.load(arr, (1, 1), (4, 8))  # Shape (4, 8), 32 elements
reshaped = ct.reshape(tile, (2, 16))  # Shape (2, 16), still 32 elements
```
"""
@inline reshape(tile::Tile{T, S}, shape::NTuple{N, Int}) where {T, S, N} =
    Intrinsics.reshape(tile, Val(shape))

"""
    permute(tile::Tile{T, S}, perm::NTuple{N, Int}) -> Tile{T, permuted_shape}

Permute the dimensions of a tile according to the given permutation.
The permutation uses 1-indexed axes (Julia convention).

# Example
```julia
tile = ct.load(arr, (1, 1, 1), (2, 3, 4))  # Shape (2, 3, 4)
# Permute axes: new_axis_1 = old_axis_3, new_axis_2 = old_axis_1, new_axis_3 = old_axis_2
permuted = ct.permute(tile, (3, 1, 2))  # Shape (4, 2, 3)
```
"""
@inline permute(tile::Tile{T, S}, perm::NTuple{N, Int}) where {T, S, N} =
    Intrinsics.permute(tile, Val(map(p -> p - 1, perm)))
@inline permute(tile::Tile{T, S}, ::Val{Perm}) where {T, S, Perm} =
    Intrinsics.permute(tile, Val(map(p -> p - 1, Perm)))

"""
    transpose(tile::Tile{T, (M, N)}) -> Tile{T, (N, M)}

Transpose a 2D tile, swapping its dimensions.
"""
@inline transpose(tile::Tile{T, S}) where {T, S} =
    Intrinsics.transpose(tile)

"""
    astype(tile::Tile{T1, Shape}, ::Type{T2}) -> Tile{T2, Shape}

Convert a tile's element type from T1 to T2.

# Example
```julia
acc = ct.full((64, 64), 0.0f0, Float32)
result = ct.astype(acc, ct.TFloat32)  # Convert to TF32 for tensor cores
```
"""
@inline astype(tile::Tile{T1, Shape}, ::Type{T2}) where {T1, Shape, T2} =
    Intrinsics.astype(tile, T2)

# Julia-style convert syntax
@inline Base.convert(::Type{Tile{T2}}, tile::Tile{T1, Shape}) where {T1, T2, Shape} =
    astype(tile, T2)

#=============================================================================
 Reduction
=============================================================================#

public reduce_sum, reduce_max

"""
    reduce_sum(tile::Tile{T, S}, axis::Integer) -> Tile{T, reduced_shape}

Sum reduction along the specified axis (1-indexed).
Returns a tile with the specified dimension removed.

# Example
```julia
# For a (128, 64) tile, reducing along axis 2:
sums = ct.reduce_sum(tile, 2)  # Returns (128,) tile
```
"""
@inline function reduce_sum(tile::Tile{T, S}, axis::Integer) where {T <: AbstractFloat, S}
    Intrinsics.reduce_sum(tile, Val(axis - 1))
end
@inline function reduce_sum(tile::Tile{T, S}, ::Val{axis}) where {T <: AbstractFloat, S, axis}
    Intrinsics.reduce_sum(tile, Val(axis - 1))
end

"""
    reduce_max(tile::Tile{T, S}, axis::Integer) -> Tile{T, reduced_shape}

Maximum reduction along the specified axis (1-indexed).

# Example
```julia
maxes = ct.reduce_max(tile, 2)  # Max along axis 2
```
"""
@inline function reduce_max(tile::Tile{T, S}, axis::Integer) where {T <: AbstractFloat, S}
    Intrinsics.reduce_max(tile, Val(axis - 1))
end
@inline function reduce_max(tile::Tile{T, S}, ::Val{axis}) where {T <: AbstractFloat, S, axis}
    Intrinsics.reduce_max(tile, Val(axis - 1))
end

#=============================================================================
 Matmul
=============================================================================#

public mma, matmul

"""
    mma(a::Tile{T1, (M, K)}, b::Tile{T2, (K, N)}, acc::Tile{T3, (M, N)}) -> Tile{T3, (M, N)}

Perform matrix-multiply-accumulate: result = a @ b + acc.
Uses tensor cores when available.
"""
@inline mma(a::Tile{T1, SA}, b::Tile{T2, SB}, acc::Tile{T3, SC}) where {T1, T2, T3, SA, SB, SC} =
    Intrinsics.mma(a, b, acc)

"""
    matmul(a::Tile{T1, S}, b::Tile{T2, S}) -> Tile{T1, S}

Perform matrix multiplication: result = a @ b.
Equivalent to `mma(a, b, zeros(output_shape, T1))`.

Supports both 2D and 3D (batched) inputs:
- 2D: a:(M, K) × b:(K, N) → (M, N)
- 3D: a:(B, M, K) × b:(B, K, N) → (B, M, N)

# Example
```julia
c = ct.matmul(a, b)  # c = a @ b
```
"""
@inline function matmul(a::Tile{T1, SA}, b::Tile{T2, SB}) where {T1, T2, SA, SB}
    _matmul(a, b, Val(length(SA)))
end

# 2D matmul: (M, K) × (K, N) → (M, N)
@inline function _matmul(a::Tile{T1, SA}, b::Tile{T2, SB}, ::Val{2}) where {T1, T2, SA, SB}
    M = SA[1]
    N = SB[2]
    acc = zeros((M, N), T1)
    mma(a, b, acc)
end

# 3D batched matmul: (B, M, K) × (B, K, N) → (B, M, N)
@inline function _matmul(a::Tile{T1, SA}, b::Tile{T2, SB}, ::Val{3}) where {T1, T2, SA, SB}
    B = max(SA[1], SB[1])  # Broadcast batch dimension
    M = SA[2]
    N = SB[3]
    acc = zeros((B, M, N), T1)
    mma(a, b, acc)
end

#=============================================================================
 Selection
=============================================================================#

public where, extract

"""
    where(cond::Tile{Bool, S}, x::Tile{T, S}, y::Tile{T, S}) -> Tile{T, S}

Element-wise conditional selection: returns x where cond is true, y otherwise.
Similar to numpy.where() or torch.where().

# Example
```julia
mask = tile_a .> tile_b  # Boolean tile
result = ct.where(mask, tile_a, tile_b)  # Element-wise max
```
"""
@inline where(cond::Tile{Bool, S}, x::Tile{T, S}, y::Tile{T, S}) where {T, S} =
    Intrinsics.select(cond, x, y)

"""
    extract(tile::Tile{T, S}, index::NTuple{N, Int}, shape::NTuple{N, Int}) -> Tile{T, shape}

Extract a sub-tile from a tile at the given slice indices.

**IMPORTANT:** The `index` parameter specifies SLICE INDICES, not element offsets!

For each dimension, the source tile is divided into `S[i] ÷ shape[i]` non-overlapping slices.
The `index[i]` selects which slice to extract (1-indexed).

# Example: Extracting quadrants from an 8×8 tile
```julia
tile = ct.load(arr, (1, 1), (8, 8))
# 8÷4 = 2 slices per dimension, so valid indices are {1, 2} × {1, 2}
tl = ct.extract(tile, (1, 1), (4, 4))  # Top-left (rows 1-4, cols 1-4)
bl = ct.extract(tile, (2, 1), (4, 4))  # Bottom-left (rows 5-8, cols 1-4)
tr = ct.extract(tile, (1, 2), (4, 4))  # Top-right (rows 1-4, cols 5-8)
br = ct.extract(tile, (2, 2), (4, 4))  # Bottom-right (rows 5-8, cols 5-8)
```
"""
@inline extract(tile::Tile{T, S}, index::NTuple{N, Int}, shape::NTuple{M, Int}) where {T, S, N, M} =
    Intrinsics.extract(tile, Val(map(i -> i - 1, index)), Val(shape))
@inline extract(tile::Tile{T, S}, ::Val{Index}, ::Val{Shape}) where {T, S, Index, Shape} =
    Intrinsics.extract(tile, Val(map(i -> i - 1, Index)), Val(Shape))

#=============================================================================
 Math
=============================================================================#

public cdiv, floordiv, sqrt, rsqrt

"""
    cdiv(a::Integer, b::Integer) -> Int32

Ceiling division: ⌈a/b⌉ = (a + b - 1) ÷ b
Useful for computing grid dimensions from array sizes and tile sizes.
"""
@inline cdiv(a::Integer, b::Integer)::Int32 = Intrinsics.cdiv(a, b)

"""
    floordiv(a::Integer, b::Integer) -> Int32

Floor division: ⌊a/b⌋
Equivalent to `a ÷ b` but provided for consistency with the cuTile API.
"""
@inline floordiv(a::Integer, b::Integer)::Int32 = Intrinsics.floordiv(a, b)

"""
    sqrt(tile::Tile{T, S}) -> Tile{T, S}

Compute element-wise square root of a tile.
"""
@inline Base.sqrt(tile::Tile{T, S}) where {T <: AbstractFloat, S} =
    Intrinsics.sqrt(tile)

"""
    rsqrt(tile::Tile{T, S}) -> Tile{T, S}

Compute element-wise reciprocal square root (1/sqrt(x)) of a tile.
"""
@inline rsqrt(tile::Tile{T, S}) where {T <: AbstractFloat, S} =
    Intrinsics.rsqrt(tile)

# Broadcasting arithmetic - different shapes, broadcast then call arith intrinsic
@inline function tile_add(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.arith(broadcast_to(a, S), broadcast_to(b, S), +)
end

@inline function tile_sub(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.arith(broadcast_to(a, S), broadcast_to(b, S), -)
end

@inline function tile_mul(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.arith(broadcast_to(a, S), broadcast_to(b, S), *)
end

@inline function tile_div(a::Tile{T, S1}, b::Tile{T, S2}) where {T <: AbstractFloat, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.arith(broadcast_to(a, S), broadcast_to(b, S), /)
end

@inline function tile_pow(a::Tile{T, S1}, b::Tile{T, S2}) where {T <: AbstractFloat, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.arith(broadcast_to(a, S), broadcast_to(b, S), ^)
end

# Scalar variants convert to 0D tile
@inline tile_add(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_add(a, Tile(b))
@inline tile_add(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_add(Tile(a), b)
@inline tile_sub(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_sub(a, Tile(b))
@inline tile_sub(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_sub(Tile(a), b)
@inline tile_mul(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_mul(a, Tile(b))
@inline tile_mul(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_mul(Tile(a), b)
@inline tile_div(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_div(a, Tile(b))
@inline tile_div(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_div(Tile(a), b)
@inline tile_div(a::Tile{T, S}, b::Integer) where {T <: AbstractFloat, S} = tile_div(a, Tile(T(b)))

# Operator overloads (same shape required)
@inline Base.:(+)(a::Tile{T, S}, b::Tile{T, S}) where {T, S} = Intrinsics.arith(a, b, +)
@inline Base.:(-)(a::Tile{T, S}, b::Tile{T, S}) where {T, S} = Intrinsics.arith(a, b, -)
@inline Base.:(*)(a::Tile{T, S}, b::Tile{T, S}) where {T, S} = Intrinsics.arith(a, b, *)
@inline Base.:(/)(a::Tile{T, S}, b::Tile{T, S}) where {T <: AbstractFloat, S} = Intrinsics.arith(a, b, /)

# Scalar-tile operators
@inline Base.:(+)(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_add(a, b)
@inline Base.:(+)(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_add(a, b)
@inline Base.:(-)(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_sub(a, b)
@inline Base.:(-)(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_sub(a, b)
@inline Base.:(*)(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_mul(a, b)
@inline Base.:(*)(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_mul(a, b)
@inline Base.:(/)(a::Tile{T, S}, b::T) where {T <: AbstractFloat, S} = tile_div(a, b)
@inline Base.:(/)(a::T, b::Tile{T, S}) where {T <: AbstractFloat, S} = tile_div(a, b)
@inline Base.:(/)(a::Tile{T, S}, b::Integer) where {T <: AbstractFloat, S} = tile_div(a, b)

# Base overloads for Int32 (special intrinsics)
@noinline Base.rem(a::Int32, b::Int32)::Int32 = Base.inferencebarrier(zero(Int32))
@noinline Base.min(a::Int32, b::Int32)::Int32 = Base.inferencebarrier(zero(Int32))

#=============================================================================
 Comparison
=============================================================================#

# Broadcasting versions - different shapes, broadcast then call cmp intrinsic
@inline function tile_lt(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), <)
end

@inline function tile_gt(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), >)
end

@inline function tile_le(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), <=)
end

@inline function tile_ge(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), >=)
end

@inline function tile_eq(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), ==)
end

@inline function tile_ne(a::Tile{T, S1}, b::Tile{T, S2}) where {T, S1, S2}
    S = broadcast_shape(S1, S2)
    Intrinsics.cmp(broadcast_to(a, S), broadcast_to(b, S), !=)
end

#=============================================================================
 Logical Operations
=============================================================================#

"""
Element-wise logical AND for boolean tiles.
"""
@inline Base.:(&)(a::Tile{Bool, S}, b::Tile{Bool, S}) where {S} =
    Intrinsics.logical_and(a, b)

#=============================================================================
 Atomic
=============================================================================#

public atomic_cas, atomic_xchg, atomic_add

"""
Memory ordering for atomic operations.
Use these constants with atomic_cas, atomic_xchg, etc.
"""
module MemoryOrder
    const Weak = 0
    const Relaxed = 1
    const Acquire = 2
    const Release = 3
    const AcqRel = 4
end

"""
Memory scope for atomic operations.
"""
module MemScope
    const Block = 0
    const Device = 1
    const System = 2
end

"""
    atomic_cas(array::TileArray, index, expected, desired; memory_order, memory_scope) -> T

Atomic compare-and-swap. Atomically compares the value at `index` with `expected`,
and if equal, replaces it with `desired`. Returns the original value.
Index is 1-indexed.

# Example
```julia
# Spin-lock acquisition
while ct.atomic_cas(locks, idx, Int32(0), Int32(1); memory_order=ct.MemoryOrder.Acquire) == Int32(1)
    # spin
end
```
"""
@inline function atomic_cas(array::TileArray{T, N}, index, expected, desired;
                            memory_order::Int=MemoryOrder.AcqRel,
                            memory_scope::Int=MemScope.Device) where {T, N}
    Intrinsics.atomic_cas(array, index - one(index), expected, desired, memory_order, memory_scope)::T
end

"""
    atomic_xchg(array::TileArray, index, val; memory_order, memory_scope) -> T

Atomic exchange. Atomically replaces the value at `index` with `val` and returns
the original value. Index is 1-indexed.

# Example
```julia
# Spin-lock release
ct.atomic_xchg(locks, idx, Int32(0); memory_order=ct.MemoryOrder.Release)
```
"""
@inline function atomic_xchg(array::TileArray{T, N}, index, val;
                             memory_order::Int=MemoryOrder.AcqRel,
                             memory_scope::Int=MemScope.Device) where {T, N}
    Intrinsics.atomic_xchg(array, index - one(index), val, memory_order, memory_scope)::T
end

"""
    atomic_add(array::TileArray, index, val; memory_order, memory_scope) -> T

Atomic addition. Atomically adds `val` to the value at `index` and returns
the original value. Index is 1-indexed.

# Example
```julia
old_val = ct.atomic_add(counters, idx, Int32(1))
```
"""
@inline function atomic_add(array::TileArray{T, N}, index, val;
                            memory_order::Int=MemoryOrder.AcqRel,
                            memory_scope::Int=MemScope.Device) where {T, N}
    Intrinsics.atomic_add(array, index - one(index), val, memory_order, memory_scope)::T
end
