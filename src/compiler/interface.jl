# Compilation interface for cuTile
#
# This file provides the public compilation API:
# - cuTileInterpreter: custom interpreter with overlay method table
# - emit_ir/emit_code: compilation callbacks (emit_binary/emit_function in CUDAExt)
# - code_tiled/@code_tiled: reflection utilities

export code_tiled, @code_tiled

using CompilerCaching: CacheView, @setup_caching, method_instance, typeinf!, results, get_source

#=============================================================================
 Interpreter
=============================================================================#

Base.Experimental.@MethodTable cuTileMethodTable

function get_method_table_view(world::UInt)
    CC.CachedMethodTable(CC.OverlayMethodTable(world, cuTileMethodTable))
end

"""
Custom interpreter that supports overlay method tables for cuTile compilation.
This is necessary because NativeInterpreter has a fixed method_table type parameter.
"""
struct cuTileInterpreter <: CC.AbstractInterpreter
    cache::CacheView
    method_table::CC.CachedMethodTable{CC.OverlayMethodTable}
    inf_cache::Vector{CC.InferenceResult}
    inf_params::CC.InferenceParams
    opt_params::CC.OptimizationParams
end

function cuTileInterpreter(cache::CacheView; always_inline::Bool=true)
    method_table = get_method_table_view(cache.world)
    inf_cache = Vector{CC.InferenceResult}()
    inf_params = CC.InferenceParams()
    opt_params = if always_inline
        CC.OptimizationParams(; inline_cost_threshold=typemax(Int))
    else
        CC.OptimizationParams()
    end
    return cuTileInterpreter(cache, method_table, inf_cache, inf_params, opt_params)
end

# Required AbstractInterpreter interface methods
CC.InferenceParams(interp::cuTileInterpreter) = interp.inf_params
CC.OptimizationParams(interp::cuTileInterpreter) = interp.opt_params
CC.get_inference_cache(interp::cuTileInterpreter) = interp.inf_cache

# World age
@static if isdefined(CC, :get_inference_world)
    CC.get_inference_world(interp::cuTileInterpreter) = interp.cache.world
else
    CC.get_world_counter(interp::cuTileInterpreter) = interp.cache.world
end

# Method table - this enables the overlays
CC.method_table(interp::cuTileInterpreter) = interp.method_table

# Locking - not needed for non-cached compilation
CC.lock_mi_inference(::cuTileInterpreter, ::MethodInstance) = nothing
CC.unlock_mi_inference(::cuTileInterpreter, ::MethodInstance) = nothing

# Setup caching - generates cache_owner and ipo_dataflow_analysis! methods
@setup_caching cuTileInterpreter.cache

# Optimization flags
CC.may_optimize(::cuTileInterpreter) = true
CC.may_compress(::cuTileInterpreter) = true
CC.may_discard_trees(::cuTileInterpreter) = true

# Disable semi-concrete interpretation (broken with overlays per JuliaLang/julia#47349)
function CC.concrete_eval_eligible(interp::cuTileInterpreter,
    @nospecialize(f), result::CC.MethodCallResult, arginfo::CC.ArgInfo, sv::CC.InferenceState)
    ret = @invoke CC.concrete_eval_eligible(interp::CC.AbstractInterpreter,
        f::Any, result::CC.MethodCallResult, arginfo::CC.ArgInfo, sv::CC.InferenceState)
    if ret === :semi_concrete_eval
        return :none
    end
    return ret
end

"""
    code_ircode(mi::MethodInstance; world, always_inline=true) -> (IRCode, rettype)

Get optimized IRCode for a MethodInstance using cuTile's overlay method table.
If always_inline=true (default), forces all functions to be inlined.
"""
function code_ircode(mi::MethodInstance; world::UInt=Base.get_world_counter(),
                     always_inline::Bool=true)
    cache = CacheView{CuTileResults}(:cuTile, world)
    interp = cuTileInterpreter(cache; always_inline)
    result = CC.typeinf_ircode(interp, mi, nothing)

    if result === nothing
        error("Type inference failed for $mi")
    end

    ir, rettype = result
    return ir, rettype
end

#=============================================================================
 Compilation phases
=============================================================================#

# Compilation options for cache sharding
const CGOpts = @NamedTuple{
    sm_arch::Union{String, Nothing},
    opt_level::Int,
    num_ctas::Union{Int, Nothing},
    occupancy::Union{Int, Nothing}
}

# Results struct for caching compilation phases
mutable struct CuTileResults
    julia_ir::Any    # (StructuredIRCode, rettype)
    tile_bc::Any     # Vector{UInt8} bytecode
    cuda_bin::Any    # Vector{UInt8} cubin (populated by CUDAExt)
    cuda_func::Any   # CuFunction (populated by CUDAExt)
    CuTileResults() = new(nothing, nothing, nothing, nothing)
end

"""
    emit_ir(cache, mi) -> (StructuredIRCode, rettype)

IR phase: run inference if needed and return structured IR.
"""
function emit_ir(cache::CacheView, mi::Core.MethodInstance)
    # Ensure CI exists
    ci = get(cache, mi, nothing)
    if ci === nothing
        interp = cuTileInterpreter(cache)
        typeinf!(cache, interp, mi)
        ci = get(cache, mi)
    end

    # Check IR cache
    res = results(cache, ci)
    res.julia_ir !== nothing && return res.julia_ir

    # Compute IR from CodeInfo
    src = @something get_source(ci)
    ir = CC.inflate_ir(src, mi)
    sci = StructuredIRCode(ir)

    res.julia_ir = (sci, ci.rettype)
    return res.julia_ir
end

"""
    emit_code(cache, mi) -> Vector{UInt8}

Code phase: generate Tile IR bytecode from StructuredIRCode.
"""
function emit_code(cache::CacheView, mi::Core.MethodInstance)
    # Delegate to previous phase (handles CI + IR)
    sci, rettype = emit_ir(cache, mi)

    # Check code cache
    ci = get(cache, mi)
    res = results(cache, ci)
    res.tile_bc !== nothing && return res.tile_bc

    # Compute bytecode
    opts = cache.owner[2]

    # Generate Tile IR bytecode
    bytecode = write_bytecode!(1) do writer, func_buf
        emit_kernel!(writer, func_buf, sci, rettype;
            name = string(mi.def.name),
            sm_arch = opts.sm_arch,
            num_ctas = opts.num_ctas,
            occupancy = opts.occupancy
        )
    end

    # Dump bytecode if JULIA_CUTILE_DUMP_BYTECODE is set
    dump_dir = get(ENV, "JULIA_CUTILE_DUMP_BYTECODE", nothing)
    if dump_dir !== nothing
        mkpath(dump_dir)
        base_filename = basename(string(mi.def.file))
        base_filename = first(splitext(base_filename))
        dump_path = joinpath(dump_dir, "$(base_filename).ln$(mi.def.line).cutile")
        counter = 1
        while isfile(dump_path)
            counter += 1
            dump_path = joinpath(dump_dir, "$(base_filename).ln$(mi.def.line).$(counter).cutile")
        end
        println(stderr, "Dumping TILEIR bytecode to file: $dump_path")
        write(dump_path, bytecode)
    end

    res.tile_bc = bytecode
    return bytecode
end

#=============================================================================
 Reflection utilities
=============================================================================#

function disassemble_tileir(bytecode::Vector{UInt8})::String
    mktempdir() do dir
        input_path = joinpath(dir, "kernel.tile")
        output_path = joinpath(dir, "kernel.disasm")
        write(input_path, bytecode)
        read(`$(cuda_tile_translate()) --cudatilebc-to-mlir $input_path`, String)
    end
end

"""
    code_typed(f, argtypes; world, kwargs...) -> Vector{Any}

Return typed code for a cuTile function.. Analogous to `Base.code_typed`.
"""
function code_typed(@nospecialize(f), @nospecialize(argtypes);
                    world::UInt=Base.get_world_counter(), kwargs...)
    cache = CacheView{CuTileResults}(:cuTile, world)
    interp = cuTileInterpreter(cache)
    Base.code_typed(f, argtypes; world, interp, kwargs...)
end

"""
    code_structured(f, argtypes; kwargs...) -> Vector{Pair{StructuredIRCode, DataType}}

Return the structured IR for a cuTile function.
"""
function code_structured(@nospecialize(f), @nospecialize(argtypes);
                         world::UInt=Base.get_world_counter(),
                         validate::Bool=true)
    cache = CacheView{CuTileResults}(:cuTile, world)
    interp = cuTileInterpreter(cache)
    map(Base.code_ircode(f, argtypes; world, interp)) do (ir, ret_type)
        StructuredIRCode(ir; validate) => ret_type
    end
end

"""
    code_tiled([io::IO], f, argtypes; sm_arch, opt_level, num_ctas, occupancy)

Print the CUDA Tile IR for a Julia function as a textual MLIR representation.
Analogous to `code_llvm`/`code_native`. Uses the compilation cache for
consistency with `launch`.
"""
function code_tiled(io::IO, @nospecialize(f), @nospecialize(argtypes);
                    sm_arch::Union{String, Nothing}=nothing,
                    opt_level::Int=3,
                    num_ctas::Union{Int, Nothing}=nothing,
                    occupancy::Union{Int, Nothing}=nothing,
                    world::UInt=Base.get_world_counter())
    tt = Base.signature_type(f, argtypes)
    if !Base.isdispatchtuple(tt)
        error("code_tiled requires a dispatch tuple, got non-concrete signature")
    end
    mi = @something(method_instance(f, argtypes; world, method_table=cuTileMethodTable),
                    method_instance(f, argtypes; world),
                    throw(MethodError(f, argtypes)))
    opts = (sm_arch=sm_arch, opt_level=opt_level, num_ctas=num_ctas, occupancy=occupancy)
    cache = CacheView{CuTileResults}((:cuTile, opts), world)
    bytecode = emit_code(cache, mi)
    print(io, disassemble_tileir(bytecode))
end
code_tiled(@nospecialize(f), @nospecialize(argtypes); kwargs...) =
    code_tiled(stdout, f, argtypes; kwargs...)

"""
    @code_tiled f(args...)

Print the Tile IR for the kernel that would be launched by the given call.
This is a convenience macro that extracts the function and argument types.

# Example
```julia
@code_tiled vadd_kernel(a, b, c)
```
"""
macro code_tiled(call)
    if !(call isa Expr && call.head === :call)
        error("@code_tiled requires a function call expression")
    end
    f = call.args[1]
    args = call.args[2:end]
    quote
        local f_val = $(esc(f))
        local args_val = ($(map(esc, args)...),)
        local argtypes = Tuple{map(typeof, args_val)...}
        code_tiled(f_val, argtypes)
    end
end
